from model import CodeBert_Seq2Seq
from utils import set_seed

LANG = "assembly"  # hoặc "python"
BASE_MODEL = "fg-codebert"                     # model nền để load config/tokenizer
BEST_CKPT = f"model/{LANG}/checkpoint-best-rouge/pytorch_model.bin"
TEST_CSV = f"code-trained/exploitgen-data/{LANG}/test.csv"
OUT_DIR = f"model/{LANG}/test_results_new"

set_seed(42)
model = CodeBert_Seq2Seq(
    ip_path=BASE_MODEL, raw_path=BASE_MODEL,
    decoder_layers=6, fix_encoder=False,
    beam_size=10, max_source_length=64, max_target_length=64,
    load_model_path=BEST_CKPT,
    layer_attention=True, l2_norm=True, fusion=True
)

model.test(test_filename=TEST_CSV, test_batch_size=16, output_dir=OUT_DIR)
print("Đã ghi kết quả vào", OUT_DIR)

# assembly
Some weights of RobertaModel were not initialized from the model checkpoint at fg-codebert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RobertaModel were not initialized from the model checkpoint at fg-codebert and are newly initializffference.
从...model/assembly/checkpoint-best-rouge/pytorch_model.bin...重新加载参数
ference.
从...model/assembly/checkpoint-best-rouge/pytorch_model.bin...重新加载参数
target: [TEST MODE]
target: [TEST MODE]
target: [TEST MODE]
source_ids length: 64
target_ids length: 64
  0%|                                                                         | 0/20 [00:00<?, ?it/s]RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
C:\Users\vuong\Documents\upload\dev1\model.py:148: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\tensor\python_tensor.cpp:80.)
  zero = torch.cuda.LongTensor(1).fill_(0)
 80%|███████████████████████████████████████████████████▏            | 16/20 [01:10<00:22,  5.72s/i 85%|██████████████████████████████████████████████████████▍         | 17/20 [01:14<00:15,  5.23s/i 90% 95%100%|████████████████████████████████████████████████████████████████| 20/20 [01:26<00:00,  4.34s/it]
{'acc': np.float64(0.5770491803278689), 'rouge-w': np.float64(0.7028212359598778)}
Đã ghi kết quả vào model/assembly/test_results_new
(.venv) PS C:\Users\vuong\Documents\upload\dev1>


# python 

Some weights of RobertaModel were not initialized from the model checkpoint at fg-codebert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RobertaModel were not initialized from the model checkpoint at fg-codebert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
从...model/python/checkpoint-best-rouge/pytorch_model.bin...重新加载参数
Test file: code-trained/exploitgen-data/python/test.csv

*** Sample test ***
idx: 0
source: initialize bad_chars to the string '\x0a\x00\x0d'...
similarity: initialize var0 to string var2...
target: [TEST MODE]
source_ids length: 64
target_ids length: 64
  0%|                                                                         | 0/24 [00:00<?, ?it/s]RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
C:\Users\vuong\Documents\upload\dev1\model.py:148: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\tensor\python_tensor.cpp:80.)
  zero = torch.cuda.LongTensor(1).fill_(0)
100%|████████████████████████████████████████████████████████████████| 24/24 [03:14<00:00,  8.11s/it]
{'acc': np.float64(0.5973333333333334), 'rouge-w': np.float64(0.767241355726272)}
Đã ghi kết quả vào model/python/test_results_ne


# assembly 2
PS C:\Users\vuong\Documents\upload\dev1> C:\Users\vuong\Documents\upload\dev1\.venv\Scripts\Activate.ps1
(.venv) PS C:\Users\vuong\Documents\upload\dev1> python .\assembly.py
Some weights of RobertaModel were not initialized from the model checkpoint at fg-codebert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RobertaModel were not initialized from the model checkpoint at fg-codebert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
从...model/assembly/checkpoint-best-rouge/pytorch_model.bin...重新加载参数
Test file: code-trained/exploitgen-data/assembly/test.csv

*** Sample test ***
idx: 0
source: _start label...
similarity: var0 label...
target: [TEST MODE]
source_ids length: 64
target_ids length: 64
  0%|                                                                                                              | 0/20 [00:00<?, ?it/s]RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
C:\Users\vuong\Documents\upload\dev1\model.py:148: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\tensor\python_tensor.cpp:80.)
  zero = torch.cuda.LongTensor(1).fill_(0)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [01:51<00:00,  5.57s/it]
{'acc': np.float64(0.49508196721311476), 'rouge-w': np.float64(0.6446646468125354)}
Đã ghi kết quả vào model/assembly/test_results_new
(.venv) PS C:\Users\vuong\Documents\upload\dev1>

from model import CodeBert_Seq2Seq
from utils import set_seed

set_seed(42)

m = CodeBert_Seq2Seq(
    ip_path="fg-codebert",
    raw_path="fg-codebert",
    decoder_layers=6,
    fix_encoder=False,
    beam_size=5,
    max_source_length=64,
    max_target_length=64,
    load_model_path="model/python/checkpoint-best-rouge/pytorch_model.bin",
    layer_attention=True,
    l2_norm=True,
    fusion=True
)

source = "add value x to variable total"
similarity = "add var0 to var1"

preds = m.predict(source, similarity)

print("Best:", preds[0])
print("Others:", preds[1:])


from model import CodeBert_Seq2Seq
from utils import set_seed

set_seed(42)

m = CodeBert_Seq2Seq(
    ip_path="fg-codebert",
    raw_path="fg-codebert",
    decoder_layers=6,
    fix_encoder=False,
    beam_size=10,
    max_source_length=64,
    max_target_length=64,
    load_model_path="model/assembly/checkpoint-best-rouge/pytorch_model.bin",
    layer_attention=True,
    l2_norm=True,
    fusion=True
)

source = "add 0x10 to the current byte in esi"
similarity = "add var0 to current byte in var1"

preds = m.predict(source, similarity)

print("Best:", preds[0])
print("Others:", preds[1:])
